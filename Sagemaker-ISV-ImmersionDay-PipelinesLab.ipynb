{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Sagemaker Immersion Day for ISVs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some pre-requisites\n",
    "## Downloading the content of the GitHub repository needed for the labs\n",
    "Please run the following cell to download the artifacts needed to your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amazon-sagemaker-immersion-day'...\n",
      "remote: Enumerating objects: 574, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 574 (delta 17), reused 5 (delta 2), pack-reused 542\u001b[K\n",
      "Receiving objects: 100% (574/574), 33.53 MiB | 40.07 MiB/s, done.\n",
      "Resolving deltas: 100% (242/242), done.\n",
      "Checking out files: 100% (96/96), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aws-samples/amazon-sagemaker-immersion-day.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipelines Lab\n",
    "### Overview\n",
    "\n",
    "**Amazon SageMaker Pipelines**, a new capability of Amazon SageMaker that makes it easy for data scientists and engineers to build, automate, and scale end to end machine learning pipelines. SageMaker Pipelines is a native workflow orchestration tool for building ML pipelines that take advantage of direct Amazon SageMaker integration. Three components improve the operational resilience and reproducibility of your ML workflows: pipelines, model registry, and projects. These workflow automation components enable you to easily scale your ability to build, train, test, and deploy hundreds of models in production, iterate faster, reduce errors due to manual orchestration, and build repeatable mechanisms.\n",
    "\n",
    "SageMaker projects introduce MLOps templates that automatically provision the underlying resources needed to enable CI/CD capabilities for your ML development lifecycle. You can use a number of built-in templates or create your own custom template (https://docs.aws.amazon.com/sagemaker/latest/dgsagemaker-projects-templates-custom.html). You can use SageMaker Pipelines independently to create automated workflows; however, when used in combination with SageMaker projects, the additional CI/CD capabilities are provided automatically. The following screenshot shows how the three components of SageMaker Pipelines can work together in an example SageMaker project.\n",
    "\n",
    "![Overview](img/image1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab focuses on using an MLOps template to bootstrap your ML project and establish a CI/CD pattern from sample code. We show how to use the built-in build, train, and deploy project template as a base for a customer churn classification example. This base template enables CI/CD for training ML models, registering model artifacts to the model registry, and automating model deployment with manual approval and automated testing.\n",
    "\n",
    "## MLOps Template for building, training and deploying models\n",
    "\n",
    "We start by taking a detailed look at what AWS services are launched when this build, train, and deploy MLOps template is launched. Later, we discuss how to modify the skeleton for a custom use case.\n",
    "\n",
    "In SageMaker Studio, you can now choose the **Projects** menu on the **Components and registries** menu.\n",
    "\n",
    "![Overview](img/image12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you choose Projects, click on Create project as below:\n",
    "\n",
    "![Overview](img/image13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the projects page, you can launch a preconfigured SageMaker MLOps template. For this lab, we choose MLOps template for model building, training, and deployment and click on Select project template\n",
    "\n",
    "![Overview](img/image14.png)\n",
    "\n",
    "In the next page provide Project Name and short Description and select Create Project.\n",
    "\n",
    "![Overview](img/image15.png)\n",
    "\n",
    "The project will take a while to be created.\n",
    "\n",
    "![Overview](img/image15bis.png)\n",
    "\n",
    "Launching this template starts a model building pipeline by default, and while there is no cost for using SageMaker Pipelines itself, you will be charged for the services launched. Cost varies by Region. A single run of the model build pipeline in us-east-1 is estimated to cost less than $0.50. Models approved for deployment incur the cost of the SageMaker endpoints (test and production) for the Region using an ml.m5.large instance.\n",
    "\n",
    "After the project is created from the MLOps template, the following architecture is deployed.\n",
    "\n",
    "![Overview](img/image16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Included in the architecture are the following AWS services and resources:\n",
    "\n",
    "* The MLOps templates that are made available through SageMaker projects are provided via an AWS Service Catalog portfolio that automatically gets imported when a user enables projects on the Studio domain.\n",
    "\n",
    "* Two repositories are added to AWS CodeCommit:\n",
    "\n",
    "    * The first repository provides scaffolding code to create a multi-step model building pipeline including the following steps: data processing, model training, model evaluation, and conditional model registration based on accuracy. As you can see in the pipeline.py file, this pipeline trains a linear regression model using the XGBoost algorithm on the well-known UCI Abalone dataset. This repository also includes a build specification file, used by AWS CodePipeline and AWS CodeBuild to run the pipeline automatically.\n",
    "\n",
    "    * The second repository contains code and configuration files for model deployment, as well as test scripts required to pass the quality gate. This repo also uses CodePipeline and CodeBuild, which run an AWS CloudFormation template to create model endpoints for staging and production.\n",
    "\n",
    "* Two CodePipeline pipelines:\n",
    "\n",
    "    * The ModelBuild pipeline automatically triggers and runs the pipeline from end to end whenever a new commit is made to the ModelBuild CodeCommit repository.\n",
    "\n",
    "    * The ModelDeploy pipeline automatically triggers whenever a new model version is added to the model registry and the status is marked as Approved. Models that are registered with Pending or Rejected statuses aren’t deployed.\n",
    "\n",
    "* An Amazon Simple Storage Service(Amazon S3) bucket is created for output model artifacts generated from the pipeline.\n",
    "\n",
    "* SageMaker Pipelines uses the following resources:\n",
    "\n",
    "    * This workflow contains the directed acyclic graph (DAG) that trains and evaluates our model. Each step in the pipeline keeps track of the lineage and intermediate steps can be cached for quickly re-running the pipeline. Outside of templates, you can also create pipelines using the SDK.\n",
    "\n",
    "    * Within SageMaker Pipelines, the SageMaker model registry tracks the model versions and respective artifacts, including the lineage and metadata for how they were created. Different model versions are grouped together under a model group, and new models registered to the registry are automatically versioned. The model registry also provides an approval workflow for model versions and supports deployment of models in different accounts. You can also use the model registry through the boto3 package.\n",
    "\n",
    "* Two SageMaker endpoints:\n",
    "    * After a model is approved in the registry, the artifact is automatically deployed to a staging endpoint followed by a manual approval step.\n",
    "    * If approved, it’s deployed to a production endpoint in the same AWS account.\n",
    "    \n",
    "    \n",
    "\n",
    "All SageMaker resources, such as training jobs, pipelines, models, and endpoints, as well as AWS resources listed in this lab, are automatically tagged with the project name and a unique project ID tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the Seed Code for Custom Use Case\n",
    "\n",
    "After your project has been created, the architecture described earlier is deployed and the visualization of the pipeline is available on the Pipelines drop-down menu within SageMaker Studio.\n",
    "\n",
    "To modify the sample code from this launched template, we first need to clone the CodeCommit repositories to our local SageMaker Studio instance. From the list of projects, choose the one that was just created. On the Repositories tab, you can select the hyperlinks to locally clone the CodeCommit repos.\n",
    "\n",
    "![repos](img/image18.png)\n",
    "\n",
    "Once both repositories have been cloned you should see the following:\n",
    "![repos](img/jma2.png)\n",
    "\n",
    "## ModelBuild Repo:\n",
    "\n",
    "The ModelBuild repository contains the code for preprocessing, training, and evaluating the model. The sample code trains and evaluates a model on [the UCI Abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone). We can modify these files to solve our own customer churn use case. See the following code:\n",
    "\n",
    "![code](img/image19.png)\n",
    "\n",
    "We now need a dataset accessible to the project.\n",
    "\n",
    "Run the following code to download a data text file and save it as a .csv in your bucket:\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt\n",
      "s3://sagemaker-us-east-1-280388799341/sagemaker/DEMO-xgboost-churn/data/RawData.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "prefix = 'sagemaker/DEMO-xgboost-churn'\n",
    "region = boto3.Session().region_name\n",
    "default_bucket = sagemaker.session.Session().default_bucket()\n",
    "RawData = boto3.Session().resource('s3')\\\n",
    ".Bucket(default_bucket).Object(os.path.join(prefix, 'data/RawData.csv'))\\\n",
    ".upload_file('./churn.txt')\n",
    "s3source=os.path.join(\"s3://\",default_bucket, prefix, 'data/RawData.csv')\n",
    "print(s3source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the pipelines directory inside the modelbuild directory and rename the abalone directory to customer_churn (or run the following cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'abalone': No such file or directory\n",
      "ImmersionDay-Sagemaker-ISV.ipynb  churn.txt\t\t\t img\n",
      "amazon-sagemaker-immersion-day\t  customer-churn-p-ibrflghsm8sb\n"
     ]
    }
   ],
   "source": [
    "!cd ../customer-churn*/*modelbuild/pipelines/; mv abalone customer_churn;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the codebuild-buildspec.yml file in the modelbuild directory and modify the run pipeline path from run-pipeline --module-name pipelines.abalone.pipeline to this:\n",
    "\n",
    " *run-pipeline --module-name pipelines.customer_churn.pipeline \\*\n",
    "\n",
    "or execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-12 23:14:29--  https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/codebuild-buildspec.yml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 993 [text/plain]\n",
      "Saving to: ‘codebuild-buildspec.yml’\n",
      "\n",
      "codebuild-buildspec 100%[===================>]     993  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-12 23:14:29 (31.0 MB/s) - ‘codebuild-buildspec.yml’ saved [993/993]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/codebuild-buildspec.yml\n",
    "!cd ../customer-churn*/*modelbuild/; mv ../../sagemaker-isv-immersionday/codebuild-buildspec.yml .; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to replace all 3 files inside the Pipeline directory as shown below;\n",
    "\n",
    "![files to replace](img/image32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the preprocess.py code under the customer_churn folder with the customer churn preprocessing script found in the sample repository.\n",
    "Replace the pipeline.py code under the customer_churn folder with the customer churn pipeline script found in the sample repository. Be sure to replace the “InputDataUrl” (line 121 of pipeline.py) default parameter with the Amazon S3 URL obtained in Step 2:\n",
    "\n",
    "\n",
    "    input_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value=f\"s3://YOUR-BUCKET/sagemaker/DEMO-xgboost-churn/data/RawData.csv\",\n",
    "    )\n",
    "\n",
    "\n",
    "The conditional step to evaluate the classification model should already be as the following:\n",
    "\n",
    "    # Conditional step for evaluating model quality and branching execution</p>\n",
    "    cond_lte = ConditionGreaterThanOrEqualTo(\n",
    "        left=JsonGet(step=step_eval, property_file=evaluation_report, json_path=\"binary_classification_metrics.accuracy.value\"), right=0.8\n",
    "    )\n",
    "\n",
    "One last thing to note is the default ModelApprovalStatus is set to PendingManualApproval. If our model has greater than 80% accuracy, it’s added to the model registry, but not deployed until manual approval is complete.\n",
    "\n",
    "Replace the evaluate.py code with the customer churn evaluation script found in the sample repository. One piece of the code we’d like to point out is that, because we’re evaluating a classification model, we need to update the metrics we’re evaluating and associating with trained models:\n",
    "\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": acc,\n",
    "                \"standard_deviation\" : \"NaN\"\n",
    "            },\n",
    "            \"auc\" : {\n",
    "                \"value\" : auc,\n",
    "                \"standard_deviation\": \"NaN\"\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    evaluation_output_path = '/opt/ml/processing/evaluation/evaluation.json'\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "\n",
    "The JSON structure of these metrics are required to match the format of sagemaker.model_metrics for complete integration with the model registry. \n",
    "\n",
    "The following cell will execute that for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-19 04:45:40--  https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/preprocess.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2726 (2.7K) [text/plain]\n",
      "Saving to: ‘preprocess.py’\n",
      "\n",
      "preprocess.py       100%[===================>]   2.66K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-19 04:45:40 (36.8 MB/s) - ‘preprocess.py’ saved [2726/2726]\n",
      "\n",
      "--2022-08-19 04:45:40--  https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/evaluate.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2694 (2.6K) [text/plain]\n",
      "Saving to: ‘evaluate.py’\n",
      "\n",
      "evaluate.py         100%[===================>]   2.63K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-19 04:45:40 (52.6 MB/s) - ‘evaluate.py’ saved [2694/2694]\n",
      "\n",
      "--2022-08-19 04:45:40--  https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/pipeline.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9919 (9.7K) [text/plain]\n",
      "Saving to: ‘pipeline.py’\n",
      "\n",
      "pipeline.py         100%[===================>]   9.69K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-19 04:45:40 (56.8 MB/s) - ‘pipeline.py’ saved [9919/9919]\n",
      "\n",
      "rm: cannot remove '/root/*.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/preprocess.py\n",
    "!wget https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/evaluate.py\n",
    "!wget https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-immersion-day/master/ML%20Pipelines%20scripts/pipeline.py\n",
    "!cat pipeline.py | sed \"s|s3://sm-pipelines-demo-data-123456789/churn.txt|$s3source|\" > pipeline-2.py\n",
    "!rm pipeline.py\n",
    "!mv pipeline-2.py pipeline.py\n",
    "!cd ../customer-churn-*/*modelbuild/pipelines/customer_churn;mv ~/sagemaker-isv-immersionday/*.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelDeploy repo:\n",
    "\n",
    "The ModelDeploy repository contains the AWS CloudFormation buildspec for the deployment pipeline. We don’t make any modifications to this code because it’s sufficient for our customer churn use case. It’s worth noting that model tests can be added to this repo to gate model deployment. See the following code:\n",
    "\n",
    "    ├── build.py\n",
    "    ├── buildspec.yml\n",
    "    ├── endpoint-config-template.yml\n",
    "    ├── prod-config.json\n",
    "    ├── README.md\n",
    "    ├── staging-config.json\n",
    "    └── test\n",
    "    ├── buildspec.yml\n",
    "    └── test.py\n",
    "\n",
    "# Triggering a pipeline run\n",
    "\n",
    "Committing these changes to the CodeCommit repository (easily done on the Studio source control tab) triggers a new pipeline run, because an Amazon EventBridge event monitors for commits. After a few moments, we can monitor the run by choosing the pipeline inside the SageMaker project.\n",
    "\n",
    "1. To commit the changes, navigate to the Git Section on the left panel and follow the steps in the screenshot below;\n",
    "    * Stage all changes\n",
    "    * Commit the changes by providing a Summary and your Name and an email address\n",
    "    * Push the changes.\n",
    "\n",
    "**Make sure you stage the Untracked changes as well.**\n",
    "![files to replace](img/image24.png)\n",
    "\n",
    "\n",
    "2. Navigate back to the project and select the Pipelines section.\n",
    "![files to replace](img/pipelines.png)\n",
    "\n",
    "\n",
    "Under execution the following screenshot shows our pipeline details.\n",
    "![files to replace](img/image25.png)\n",
    "\n",
    "3. If you double click on the executing pipelines, the steps of the pipeline will appear. You will be able to monitor the step that is currently running.\n",
    "![files to replace](img/image26.png)\n",
    "\n",
    "![files to replace](img/image27.png)\n",
    "\n",
    "\n",
    "4. When the pipeline is complete, you can go back to the project screen and choose the Model groups tab. You can then inspect the metadata attached to the model artifacts.\n",
    "![files to replace](img/image28.png)\n",
    "\n",
    "5. If everything looks good, you can click on the Update Status tab and manually approve the model.\n",
    "![files to replace](img/image29.png)\n",
    "\n",
    "![files to replace](img/image30.png)\n",
    "\n",
    "![files to replace](img/image30-2.png)\n",
    "\n",
    "\n",
    "You can then go to **Endpoints** in the SageMaker menu.\n",
    "![files to replace](img/image30-3.png)\n",
    "\n",
    "You will see a staging endpoint being created.\n",
    "![files to replace](img/image30-4.png)\n",
    "\n",
    "After a while the endpoint will be listed with the **InService** status.\n",
    "![files to replace](img/image30-5.png)\n",
    "\n",
    "To deploy the endpoint into production, you need to put your \"DevOps Team\" hat and go to CodePipeline.\n",
    "![files to replace](img/image30-6.png)\n",
    "\n",
    "Click on the modeldeploy pipeline which is currently in progress.\n",
    "![files to replace](img/image30-7.png)\n",
    "\n",
    "At the end of the DeployStaging phase, you need to manually approve the deployment.\n",
    "![files to replace](img/image30-8.png)\n",
    "\n",
    "Once it is done you will see the production endpoint being deployed in the SageMaker Endpoints.\n",
    "![files to replace](img/image30-9.png)\n",
    "\n",
    "After a while the endpoint will also be InService.\n",
    "![files to replace](img/image31.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi tenancy\n",
    "\n",
    "Lets create a second training dataset for our second customer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt\n",
      "s3://sagemaker-us-east-1-280388799341/sagemaker/DEMO-xgboost-churn-cust2/data/RawData.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'sagemaker/DEMO-xgboost-churn-cust2'\n",
    "RawData = boto3.Session().resource('s3')\\\n",
    ".Bucket(default_bucket).Object(os.path.join(prefix, 'data/RawData.csv'))\\\n",
    ".upload_file('./churn.txt')\n",
    "s3sourcecust2=os.path.join(\"s3://\",default_bucket, prefix, 'data/RawData.csv')\n",
    "print(s3sourcecust2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline with a different training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=boto3.client('sagemaker')\n",
    "response = client.start_pipeline_execution(\n",
    "    PipelineName='arn:aws:sagemaker:us-east-1:280388799341:pipeline/customer-churn-p-ibrflghsm8sb',\n",
    "    PipelineParameters=[\n",
    "        {\n",
    "            'Name': 'InputDataUrl',\n",
    "            'Value': s3sourcecust2\n",
    "        },\n",
    "    ],\n",
    "    PipelineExecutionDescription='customer2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab we have walked through how a data scientist can modify a preconfigured MLOps template for their own modeling use case. Among the many benefits is that the changes to the source code can be tracked, associated metadata can be tied to trained models for deployment approval, and repeated pipeline steps can be cached for reuse. To learn more about SageMaker Pipelines, check out the website and the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
